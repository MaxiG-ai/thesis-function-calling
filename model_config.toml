# --- Model Registry ---
# Static definitions of available models and their capabilities.

[models.gpt-5]
litellm_name = "openai/gpt-5"
context_window = 128000
provider = "aicore"
api_base = "http://localhost:4000/v1"
api_key = "THINKTANK"

[models.gpt-5-mini]
litellm_name = "openai/gpt-5-mini"
context_window = 128000
provider = "aicore"
api_base = "http://localhost:4000/v1"
api_key = "THINKTANK"

[models.gpt-4-1]
litellm_name = "openai/gpt-4.1"
context_window = 128000
temperature = 0
provider = "aicore"
api_base = "http://localhost:4000/v1"
api_key = "THINKTANK"

[models.gpt-4-1-mini]
litellm_name = "openai/gpt-4.1-mini"
context_window = 128000
temperature = 0
provider = "aicore"
api_base = "http://localhost:4000/v1"
api_key = "THINKTANK"

[models.gpt-4o-2024-08-06]
litellm_name = "openai/gpt-4o-2024-08-06"
context_window = 128000
provider = "aicore"
api_base = "http://localhost:4000/v1"
api_key = "THINKTANK"


[models.glm-4-9b]
litellm_name = "ollama/glm4:9b"
context_window = 128000
provider = "ollama"
api_base = "http://localhost:4000/v1"
api_key = "THINKTANK"

[models.llama-31-7b]
litellm_name = "ollama/llama3.1"
context_window = 128000
provider = "ollama"
api_base = "http://localhost:4000/v1"
api_key = "THINKTANK"

[models.gpt-oss-20b]
litellm_name = "ollama/gpt-oss:20b"
context_window = 128000
provider = "ollama"
api_base = "http://localhost:4000/v1"
api_key = "THINKTANK"