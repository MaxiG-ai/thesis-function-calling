# --- Model Registry ---
# Static definitions of available models and their capabilities.

[models.llama3-small-local]
litellm_name = "ollama/llama3.2:3b"
context_window = 128000
provider = "ollama"

[models.gemma-27b-free]
litellm_name = "openrouter/google/gemma-3-27b-it:free"
context_window = 8192
provider = "openrouter"

[models.gpt-5-mini]
litellm_name = "openai/gpt-5-mini"
context_window = 128000
provider = "aicore"
api_base = "http://localhost:3030/v1"     # ðŸ‘ˆ Point to your proxy (ensure /v1 is correct for your setup)
api_key = "THINKTANK"                  # Proxies usually require a non-empty string
